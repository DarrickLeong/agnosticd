= ocp4_workload_example_dedicated_cluster - Example Workload Role for a Dedicated Cluster

== Preparation to Use This Role

=== Get Access to Your Dedicated Cluster

This example role assumes you'll be using the Shared Cluster provided by GPTE.
There should be a catalog item in CloudForms to give you access and get stated developing.

== Description

This role builds upon `ocp4-workshop-example-shared-cluster` to prepare a dedicated cluster and deploy the sample application for multiple users.
Admin access to the cluster is required.
This role is meant to be run after serveral important depenendent roles, including setup of 
* Let's Encrypt certificates
* htpasswd authentication for multiple users
* a project request template
Dependent workload roles can be executed several different ways: on the command line from config files, as part of an AgnosticV configuration, or through CloudForms and any other catalog.
The role is meant to trigger the deployment of the AgnosticD env_config: `ocp4-cluster`, but it can work against any dedicated OpenShift cluster with `system:admin` access.

When removed by the deployer (CloudForms or command line), this role deletes resources it created for all the users in OpenShift.
****
For more information on dedicated clusters and setting up self-provisioner capable users and quotas on your own dedicated clusters, see "ocp4_workload_shared_cluster_access"
****

== Role overview

* This is a working simple role that can be used to develop new OpenShift 4 workload roles on dedicated clusters. It consists of the following playbooks:

=== Playbooks

** Playbook: link:./tasks/pre_workload.yml[pre_workload.yml] - Sets up an environment for the workload deployment
*** Debug task will print out: `pre_workload Tasks completed successfully.`

** Playbook: link:./tasks/workload.yml[workload.yml] - Used to deploy the actual workload, i.e, 3scale, Mobile, some Demo or OpenShift customization
*** Read this role for our recommended best practices for working with ansible and OpenShift
*** This role creates an OpenShift project, deploys a simple application, and notifies on success or failure.
*** Debug task will print out: `workload Tasks completed successfully.`

** Playbook: link:./tasks/post_workload.yml[post_workload.yml] - Used to configure the workload after deployment
*** This role doesn't do anything
*** Debug task will print out: `post_workload Tasks completed successfully.`

** Playbook: link:./tasks/remove_workload.yml[remove_workload.yml] - Used to delete the workload
*** This role deletes the OpenShift projected created for the user.
*** Debug task will print out: `remove_workload Tasks completed successfully.`

=== The defaults variable file

* This file link:./defaults/main.yml[./defaults/main.yml] contains all the variables you need to define to control the deployment of your workload.
* The variable *ocp_username* is mandatory to assign the workload to the correct OpenShift user.
* A variable *silent=True* can be passed to suppress debug messages.
* You can override any of these default values by adding `-e "variable_name=variable_value"` to the command line or through your deployer.
* Note that this role users long-name scoped workload parameters. Example: `ocp4_workload_example_dedicated_cluster_project_name: my-project-name`, not just `project_name`

=== Understand how paremeters to your workload should be specified

Please use long-name scope parameters for your workload.
Ansible lacks robust variable scoping.
For parameters passed into your workload role, prepend the role name.
For parameters shared across workloads, have a catch-all name that applies to your goal.
For example, parameters named `ocp4_workload_example_dedicated_cluster_*` would apply to workloads `ocp4-workload-example_dedicated_cluster` itself *and* `ocp4_workload_example_dedicated_cluster_minio`.

=== Provide info to users

In your playbooks, emit as part of a `debug` play the `user.info` variables to provide important info back to users via email.

.For example:
----
- name: output workshop username distribution URL
  debug:
    msg:
    - "user.info: "
    - "user.info: {{ ocp_username }} is your username. "
    - "user.info: "
----

In RHPDS and OpenTLC, CloudForms will pick up this information and email it to your user.

This example workload is geared for use with the existing GPTE Shared OCP clusters, or your own clusters that you maintain for various purposes.
Your workload might be an OpenShift demo on RHPDS's shared clustesr, or one of many workloads all used for a demo or workshop.

== Deploy a Workload from the command line with the `ocp-workload` playbook [Mostly for testing]

TODO: explain getting share cluster access via OpenTLC/RHDPS, etc.
TODO: explain deploying from CloudForms - `workload_shared_deployment: true`

. If your workload uses parameters create a `<role name>_vars.yaml` input file.
+
.ocp4_workload_example_dedicated_cluster_vars.yaml
[source,yaml]
----
# You can set any variable
silent: true

# Set a variable scoped to the role.  In this case, a variable for a project name to be created.
ocp4_workload_example_dedicated_cluster_project_name: "sample-application-{{ ocp_username }}"
----

. Set up Environment Variables for the bastion you want to run this role on.
+
[source,bash]
----
TARGET_HOST="bastion.dev.openshift.opentlc.com"
OCP_USERNAME="wkulhane-redhat.com"
ANSIBLE_USER="ec2-user" # Will become OpenTLC username
WORKLOAD="ocp4_workload_example_dedicated_cluster"
GUID="1001"
----

. Finally run the workload passing the input files as parameters:
+
[source,sh]
----
# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
    -e"ansible_ssh_private_key_file=~/.ssh/keytoyourhost.pem" \
    -e"ansible_user=${ANSIBLE_USER}" \
    -e"ocp_username=${OCP_USERNAME}" \
    -e"ocp_workload=${WORKLOAD}" \
    -e"guid=${GUID}" \
    -e"ACTION=create" \
    -e @./ocp4_workload_example_dedicated_cluster_vars.yaml \
    -e @./ocp4_workload_example_dedicated_cluster_secrets.yaml
----
+

=== To Delete a Workload from the CLI

----
TARGET_HOST="bastion.dev.openshift.opentlc.com"
OCP_USERNAME="wkulhane-redhat.com"
ANSIBLE_USER="ec2-user" # Will become OpenTLC username
WORKLOAD="ocp_workload_example_dedicated_cluster"
GUID="1001"

# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i ${TARGET_HOST}, ./configs/ocp-workloads/ocp-workload.yml \
    -e"ansible_ssh_private_key_file=~/.ssh/keytoyourhost.pem" \
    -e"ansible_user=ec2-user" \
    -e"ocp_username=${OCP_USERNAME}" \
    -e"ocp_workload=${WORKLOAD}" \
    -e"guid=${GUID}" \
    -e"ACTION=remove" \
    -e @./ocp_workload_example_dedicated_cluster_vars.yaml \
    -e @./ocp_workload_example_shared_clsuter_secrets.yaml
----


== Deploying a Workload with AgnosticV from the Command Line

When creating a configuration in AgnosticV that includes the deployment of the workload you can specify the variables straight in the AgnosticV config.
AgnosticV configs are usually created by combining a `common.yaml` file with either `dev.yaml`, `test.yaml` or `prod.yaml`.
You can specify different variables in each of these files.
For example you could have common values defined in the `common.yaml` file and then specific values overriding the common ones for development or production environments in `dev.yaml` or `prod.yaml`.

AgnosticV merges the definition files starting with `common.yaml` and then adding/overwriting what comes from either `dev.yaml` or `prod.yaml`.

Example of a simple AgnosticV config:

.common.yaml
[source,yaml]
----
# --- Example Shared Cluster Workload Deployment for RPDS
# --- System: RHPDS
# --- Catalog: OpenShift Demos
# --- Catalog Item: Quay 3 on OpenShift 4

# --- Platform
platform: rhpds

# --- Cloud Provider
cloud_provider: none

# --- Config
env_type: ocp4-cluster
ocp_workload: ocp4_workload_example_dedicated_cluster
# If your workload requires sudo, additional privileges are required.  
# For now, workload must be run as ec2-user (or cloud-user on OpenStack)
ansible_user: ec2-user
ansible_ssh_private_key_file: /home/opentlc-mgr/.ssh/opentlc_admin_backdoor.pem

# --- Ensure the workload prints the correct statements for CloudForms to realize it finished
workload_shared_deployment: true

# --- Workload Configuration
ocp4_workload_example_dedicated_cluster_project_name: "ocp4-workload-example-{{ ocp_username }}"

# --- AgnosticV Meta variables
agnosticv_meta:
  params_to_variables:
    user: ocp_username
  secrets:
  # A secret file that might hold something your role needs
  - ocp4_workload_example_dedicated_cluster_secrets.yml
----

.dev.yaml
[source,yaml]
----
purpose: development

# --- Use specific variable values for Development
target_host: bastion.dev4.openshift.opentlc.com

# --- Workload Configuration Overrides
ocp4_workload_example_dedicated_cluster_project_name:  "ocp4-workload-example-{{ ocp_username }}-dev"
----

.prod.yaml
[source,yaml]
----
---
purpose: production

# --- Use specific variable values for Production
target_host: bastion.rhpds.openshift.opentlc.com

# --- Workload Configuration Overrides
ocp4_workload_example_dedicated_cluster_project_name:  "ocp4-workload-example-{{ ocp_username }}-prod"

# --- AgnosticV Meta variables
agnosticv_meta:
  agnosticd_git_tag_prefix: ocp4-workload-example-shared-cluster-prod
----

== Further Learning - More Complex Examples

If you want to see more examples of how this works in a real world workload the following workloads already use this approach:

* ocp4_workload_example_dedicated_cluster
* ocp4_workload_authentication
* ocp4_workload_machinesets
* ocp4_workload_logging
* ocp4_workload_quay_operator

////
=== User Authentication on Dedicated Clusters

NOTE: Use only for dedicated cluster.  Do not use on a Shared Cluster.

The `ocp4_workload_authentication` role allows you to setup the authentication system that makes sense for you:
. link:https://github.com/redhat-cop/agnosticd/blob/development/ansible/roles_ocp_workloads/ocp4_workload_authentication/defaults/main.yml

Use the `ocp4_workload_dedicated_cluster_access` workload to give a user access to the shared cluster.

.Add the following variables, and the workload will give access and create a quota:
----
ocp_workload: ocp4_workload_dedicated_cluster_access
ocp_username: <your OpenTLC username>
----

.Here are the details of the quota created by the workload:
https://github.com/redhat-cop/agnosticd/blob/development/ansible/roles_ocp_workloads/ocp4_workload_dedicated_cluster_access/defaults/main.yml

////


== AgnosticV Samples

.AgnosticV common.yaml
----
---
# Platform
platform: RHPDS

# Config
env_type: ocp4-workshop
install_ocp4: true
software_to_deploy: none
ocp4_installer_version: "4.3.21"
install_lets_encrypt_certificates: true
enable_workshops_catalog: true
# ignore_self_provisioners: true
install_ipa_client: false
run_ocp_diagnostics: false
key_name: ocpkey
# install_ipa_client and ipa_host_password passed from cloudforms

# test user base var in agnosticd
# ocp4_workload_integreatly_user_base: 0

# Integreatly-specific config
ocp4_workload_integreatly_user_count: 50
ocp4_workload_integreatly_user_base: evals
ocp4_workload_integreatly_admin_username: admin
ocp4_workload_integreatly_user_password: openshift
ocp4_workload_integreatly_dedicated_admin_user_password: openshift
ocp4_workload_integreatly_admin_user_password: openshift
ocp4_workload_authentication_htpasswd_admin_password: openshift

# subdomain_base_suffix and HostedZoneIdcome from the account
#subdomain_base_suffix: .open.redhat.com
#HostedZoneId: ZCRCH49OF7I9E

# emergency fix for account issue:
subdomain_base_suffix: .example.opentlc.com
HostedZoneId: Z3IHLWJZOU9SRT

# Repos
ig_version: 2.2.0
osrelease: 4.3.0
# osrelease and ig_version passed from cloudforms dialog parameter
repo_method: file

# OCP Authentication
admin_user: admin
ocp4_workload_authentication_idm_type: htpasswd
ocp4_workload_authentication_htpasswd_user_base: evals
ocp4_workload_authentication_htpasswd_user_password: openshift
ocp4_workload_authentication_htpasswd_user_count: 50
ocp4_workload_authentication_admin_user: admin
ocp4_workload_authentication_admin_user_password: r3dh4t1!
ocp4_workload_authentication_remove_kubeadmin: true

infra_workloads:
  - ocp4_workload_le_certificates
  - ocp4_workload_integreatly_minio
  - ocp4_workload_authentication
  - ocp4_workload_integreatly

agnosticv_meta:
  virtualenv: aws-ansible-2.9
  params_to_variables:
    username: student_name
    letsencrypt: install_lets_encrypt_certificates
    infra_workloads: infra_workloads
    student_workloads: student_workloads
  secrets:
    - gpte
    - ocp4_token
----

.AgnosticV dev.yaml
----

---
lets_encrypt_production: false
run_ocp_diagnostics: false
cloudformation_retries: 0

#"AWS instance types"
bastion_instance_type: t3.medium
_infra_node_instance_type: m5.4xlarge
_infra_node_replicas: 3
clientvm_instance_type: t2.medium
clientvm_instance_count: 1
master_instance_type: m5.2xlarge
master_instance_count: 3
worker_instance_type: m4.2xlarge
worker_instance_count: 10
support_instance_type: t3.medium

agnosticv_meta:
  idle_after_deploy: 10
  idle_after_start: 10
  params_to_variables:
    nodecount: worker_instance_count_ignored
----

.AgnosticV test.yaml
----

---
lets_encrypt_production: true
run_ocp_diagnostics: false
cloudformation_retries: 0

#"AWS instance types"
bastion_instance_type: t3.medium
_infra_node_instance_type: m5.4xlarge
_infra_node_replicas: 3
clientvm_instance_type: t2.medium
clientvm_instance_count: 1
master_instance_type: m5.2xlarge
master_instance_count: 3
worker_instance_type: m4.2xlarge
worker_instance_count: 5
support_instance_type: t3.medium

agnosticv_meta:
  idle_after_deploy: 10
  idle_after_start: 10
  params_to_variables:
    nodecount: worker_instance_count_ignored
  agnosticd_git_tag_prefix: ocp4-workload-integreatly-test
----

.AgnosticV prod.yaml
----

---
purpose: production
lets_encrypt_production: true

run_ocp_diagnostics: false
cloudformation_retries: 0

#"AWS instance types"
bastion_instance_type: t3.medium
_infra_node_instance_type: m5.4xlarge
_infra_node_replicas: 3
clientvm_instance_type: t2.medium
clientvm_instance_count: 1
master_instance_type: m5.2xlarge
master_instance_count: 3
worker_instance_type: m4.2xlarge
worker_instance_count: 10
support_instance_type: t3.medium

agnosticv_meta:
  idle_after_deploy: 9999
  idle_after_start: 9999
  params_to_variables:
    nodecount: worker_instance_count_ignored
  agnosticd_git_tag_prefix: ocp4-workload-integreatly-prod
----
