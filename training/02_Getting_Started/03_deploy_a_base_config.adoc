= How to deploy a base config on Agnostic D from an OpenStack Sandbox

Assuming you’ve correctly configured your vars files, it’s time to deploy "a base config". (Make sure your virtualenv is active, your prompt should start `(openstack) `).

==== Preparing your a-base-config vars
The AgnosticD config a-base-config comes with sample vars files included for OpenStack, locally at ~/agnosticd/ansible/configs/a-base-config/sample_variables/rhel8_server_on_osp.yml. Only a few variables need changed.

1. Copy the `rhel8_server_on_osp.yml` file and call it `my_vars.yml`.
+
[source,bash]
----
$ cp configs/a-base-config/sample_variables/rhel8_server_on_osp.yml \
  ~/my_vars.yml
----

2. Edit your copy of the vars file, changing the value of guid to a valid subdomain name (For example an alphanumeric string starting with a letter). The values that need changing are identified below:
+
[source,bash]
----
$ vi ~/my_vars.yml
cloud_provider: osp                  # This var file is meant for an openstack deployment
env_type: a-base-config              # Name of the config to deploy
software_to_deploy: none             # Not deploying any software onto the environment

guid: CHANGE_ME

# Student User ID
student_name: CHANGE_ME              # Change to guid of the environment. This will be used to create a student login ID
student_password: "password"         # Customize the student password here. Keep in mind these systems may be public facing.
admin_user: CHANGE_ME                # Change to the guid of the environment
output_dir: /tmp/output_dir          # Writable working scratch directory
email: CHANGE_ME                     # User info for notifications
----

WARNING: Do not pick the same GUID as the one you got for access to the OSP cluster.

3. Execute the main AgnosticD playbook (bear in mind the path to your files, which may differ):
+
[source,bash]
----
(openstack) [agilpipp-redhat.com@bastion ansible]$ ansible-playbook main.yml \
   -e @~/my_vars.yml \ 
   -e @~/secrets.yml
----
+

NOTE: If you are having python2 Vs. Python3 issues, Add `/usr/bin/python3.6` before the ansible-playbook command. For example: `/usr/bin/python3.6 ansible-playbook ansible/main.yml -e @~/my_vars.yml -e@~/secrets.yml`

4. Finding your new servers

Congratulations!
You should now have your base config deployed succesfully.

In the logs that scrolled by you may have noticed the IP address of your new bastion server. This, and your other instance(s), have been captured in a temporary working directory, the `output_dir` set in your `~/my_vars.yml` file typically set to `/tmp/output_dir` if you have not changed it previously. This contains, amongst other files, an ssh configuration file.

Append this file to your existing ~/.ssh/config in the case you are using a sandbox instance:
+
[source,bash]
----
$ cat /tmp/output_dir/*_ssh_conf >> ~/.ssh/config
----

. Check that the VM was installed and ssh into the box using the created key. In order to do so, first log into your bastion, using the credentials provided to you via email:
+
[source,bash]
----
sassenach:~ Cibeles$ ssh YOUR-OPENTLC-USER@bastion.GUID.red.osp.opentlc.com
YOUR-OPENTLC-USER@bastion.GUID.red.osp.opentlc.com's password: YOUR_EMAIL_PROVIDED_PASSWORD
----

+
[source,bash]
----
[YOUR_USER@bastion ~]$ openstack --os-cloud=${GUID}-project server list

+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| ID                                   | Name    | Status | Networks                                                | Image | Flavor  |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
| 2715f0d9-51e1-4619-a97e-c841914dddf6 | node    | ACTIVE | testamaya-default-network=192.168.47.26                 |       | 2c2g30d |
| 6931bf5a-ec1e-4ac7-8477-9e96f9e14de3 | bastion | ACTIVE | testamaya-default-network=192.168.47.17, 169.47.188.156 |       | 2c2g30d |
| 947d6397-c152-4a38-9825-02f9fa50c03e | bastion | ACTIVE | 98e1-testnet-network=192.168.0.35, 169.47.191.80        |       | 2c2g30d |
+--------------------------------------+---------+--------+---------------------------------------------------------+-------+---------+
----

You can see there are some machines there and their IP addresses. Now you can log into your bastion machine from the outside world (your laptop) or from the bastion machine you were given credentials for in the email.

Let's log in from the outside world (your laptop) using your ${GUID}_infra_ssh_key.pem key file. Please note that the key file should be created in the machine you launched the playbook from.

[source,bash]
----
sassenach:~ Cibeles$ ll /tmp/output_dir/
 8 -rw-r--r--   1 Cibeles  staff   235B May 26 17:41 basic_heat_template.yml
 8 -rw-r--r--   1 Cibeles  staff   369B May 28 16:38 hosts-just-a-bunch-of-nodes-testamaya
16 -rw-r--r--   1 Cibeles  staff   6.8K May 26 17:42 just-a-bunch-of-nodes.testamaya.osp_cloud_master_template.yaml
 8 -rw-r--r--   1 Cibeles  staff   1.1K May 28 16:32 just-a-bunch-of-nodes_testamaya_ssh_conf
 8 -rw-r--r--   1 Cibeles  wheel   175B May 28 16:33 just-a-bunch-of-nodes_testamaya_ssh_known_hosts
 8 -rwxr--r--   1 Cibeles  staff   168B May 28 16:37 ssh-config-just-a-bunch-of-nodes-testamaya*
 8 -rw-------   1 Cibeles  staff   1.6K May 25 13:16 testamaya_infra_ssh_key.pem
 8 -r--------   1 Cibeles  wheel   1.8K May 28 16:19 testamayakey
 8 -rw-r--r--   1 Cibeles  wheel   399B May 28 16:19 testamayakey.pub
 
sassenach:~ Cibeles$ ssh -i /tmp/output_dir/testamaya_infra_ssh_key.pem cloud-user@169.47.188.156
Last login: Thu May 28 10:49:27 2020 from 90.77.177.210
[cloud-user@bastion 0 ~]$
----

. Now, if you want to log into any of your nodes, you just simply need to copy the identity file into the newly deployed bastion machine and ssh from it (as nodes do not have an external IP).
+
[source,bash]
----
sassenach:~ Cibeles$  scp -i /tmp/output_dir/testamaya_infra_ssh_key.pem /tmp/output_dir/testamaya_infra_ssh_key.pem cloud-user@169.47.188.156:.

[cloud-user@bastion ~]$ ssh -i testamaya_infra_ssh_key.pem cloud-user@node
Last login: Tue Jun  2 12:16:17 2020 from bastion.example.com
----

. You can now adapt `my_vars.yml` to your needs. Create different kind of instances, more security groups, etc.

=== Clean up

. Destroy the deployment:
+
[source,bash]
----
ansible-playbook destroy.yml \
  -e @configs/just-a-bunch-of-nodes/my_vars.yml \
  -e @~/secret.yml
----

=== What's next ?

- link:../ansible/configs/ocp-workloads[ocp-workloads]: deploy an OpenShift app on a shared cluster. See link:../ansible/configs/ocp-workloads/sample_vars[`sample_vars`] directory.
- link:../ansible/configs/ocp4-cluster[ocp4-cluster]: deploy an OpenShift cluster. You can applied your workloads on top of it using the `ocp_workloads` list. See link:../ansible/configs/ocp4-cluster/sample_vars_osp.yml[sample_vars_osp.yml].
